\chapter{Ciele práce}

Agent ako jednotka schopná konať rozhodnutia (akcie) v prostredí danom Markovovim \cite{bib:markov_02}
rozhodovacím procesom hľadá optimálnu stratégiu v zmysle rovnice \ref{eq:q_quality}.
Cieľom agenta je teda nájsť optimálnu stratégiu a maximalizovať tak odmenu.
Pre veľký počet stavov je hľadanie optima metódou počítania
pravdepodobností prechodov medzi stavmi $P(s, s')$ ťažko vypočítateľné.

Východiskom sú napríklad algoritmy Q-learning, alebo SARSA. Tieto algoritmy počítajú
ohodnotenie akcie v danom stave $Q(s(n), a(n))$, ktoré číselne vyjadruje vhodnosť
danej akcie. Využitie môžu nájsť \cite{bib:q_app_01}, \cite{bib:q_app_02}, \cite{bib:q_app_03} napríklad pri plánovnaí rozhodnutí v
\begin{enumerate}
  \item robotike
  \item virtuálnych agentových systémoch
  \item počítačové hry
\end{enumerate}

Vo všobecnosti riešia uvedené algoritmy problémy umelej inteligencie, kedy
nie je možné zostaviť trénovacie dáta v tvare vstup, požadovaný výstup a aplikácia
je obmedzená na udeľovanie odmien agentovi za vykonanie zvolenej stratégie
\cite{bib:reinforcement_leraning_01}, \cite{bib:reinforcement_leraning_02}. Na rozdiel
od evolučných algoritmov (genetické algoritmy, diferenciálna evolúcia, simulované žíhanie),
kedy je daná kriteriálna funkcia, umožňujú algoritmy Q-learning, alebo SARSA
postupne zlepšovať riešenie na princípe hľadania optimálnej stratégie z niekoľkých
optimálnych podstratégií - už nájdené optimálne riešenie podstratégie sa nemení. V prípade evolučných
algoritmov je typická zmena všetkých hľadaných parametrov. Nie sú teda vhodné
na úlohy kde sa požaduje generovanie postupnosti akcií.

Pre algoritmus Q-learning je zaručená konvergencia k optimálnemu
ohodnoteniu (v zmysle \ref{eq:q_quality}) \cite{bib:q_conv_proof}
pre ľubovolnú metódu výberu
akcií - postačuje aby každá akcia mala nenulovú pravdpodobnosť vykonania v prislúchajúcom
stave. V prípade SARSA táto konvergencia nie je zaručená pre všetky metódy výberu akcií.
Oba algoritmy pracujú v diskrétnom čase.

Pre problémy s rádovo stovkami stavov, ktoré sú diskrétne, môže byť fukcia $Q(s(n), a(n))$ realizovaná
formou tabuľky. Konvergencia k optimálnemu riešeniu je v tomto prípade zaručená.
Pre problémy kde je počet stavov veľmi veľký (tisíce a viac), alebo stavy nenadobúdajú
diskrétne hodnoty je potrebné zvoliť aproximáciu tejto funkcie. Konvergencia v tomto
prípade už nie je zaručená.

Prístupov ako aproximovať túto funkciu je niekoľko \cite{bib:aproximation_01},
\cite{bib:aproximation_02}, \cite{bib:aproximation_03}, \cite{bib:aproximation_04}.
Najčastejšie používané
\begin{enumerate}
  \item Diskretizácia stavov spojitých hodnôt tabuľkou
  \item Lineárna kombinácia príznakov
  \item Dopredná neurónová sieť
  \item Neurónová sieť bázických funkcií
\end{enumerate}

Prvý spôsob predstavuje triviálne riešnie problému redukciou nekonečného
počtu stavov na konečný.

Druhý spôsob spočíva v pevne definovaných príznakoch, ktoré závisia od typu
problému. Tieto príznaky tvoria súbor funkcií $f_{i}(s(n),a(n))$. Hodnota $Q(s(n), a(n))$
je daná lineárnou kombináciou týchto príznakov. Hľadá sa teda vektor váh
$w$ pre ktorý  $Q_b(s(n), a(n), w) = \sum\limits_{i=0}^{I}w_i f_{i}(s(n),a(n))$
má minimálnu veľkosť chyby $e$, definovaná je ako
$e(w) = \sum\limits_{s,a} (Q(s(n), a(n))- Q_b(s(n), a(n), w))^2$
Problematická zostáva voľba príznakových funkcií - ich tvar aj počet.

Tretí spôsob spočíva v použití doprednej neurónovej siete ako univerzálny aproximátor funkcie.
Schopnosť aproximovať funkciu doprednou neurónovou sieťou je veľmi dobre známa aj preskúmaná.
Pre úlohy Q-learning algoritmu je však nepoužiteľná \cite{bib:q_fnn_problem},
z dôvodov nemožnosti túto sieť naučiť doteraz dostupnými prostriedkami. Hoci existuje niekoľko prípadov kde sa učenie dá
uskutočniť, vo všeobecnosti sú v protiklade dva požiadavky :
\begin{enumerate}
  \item Učenie siete na požadovanú hodnotu
  \item Generovanie požadovanej hodnoty
\end{enumerate}

Sieť teda musí zároveň poskytovať správny výstup pre minulé stavy a zároveň sa
učiť na súčastný stav bez toho, aby sa hodnoty z minulých stavov zmenili.

Štvrtý spôsob je využíva lineárnu kombináciu bázických funkcie.
Bázické funkcie sú dané vopred, avšak ich parametre sa menia v priebehu učenia,
podobne ako vektor váh lineárnej kombinácie $w$. Nech sú ich parametre označené
ako $v$. Cieľom je nájsť také $w$ a $v$ pre ktoré chyba
$e(v, w) = \sum\limits_{s,a}(Q(s(n), a(n))- Q_b(s(n), a(n), v, w))^2$
je minimálna. Kde $Q_b(s(n), a(n), v, w) = \sum\limits_{i=0}^{I}w_i f_{i}(s(n),a(n), v_i)$.

{\bf Cieľom práce} je overiť možnosti aproximácie funkcie $Q(s(n), a(n))$
uvedenými metódami. Vzľadom na už prebehnutý výskum a problémy dopredných
neurónových sieti, sa problematika sústredí najmä
na hľadanie vhodných bázických funkcií. Práve v tejto oblasti je venovaný výskumu
najväčší priestor. Tieto funkcie by mali byť volené tak, aby zmena parametrov $v_i$ jednej
funkcie, neovplivnila výsledok inde ako pre žiadané $s(n)$ a $a(n)$.
Použité riešnie je potom možné využiť vo veľkých stavových priestorov, kde možnosti použiť tabuľku
zlyhávajú z dôvodov
\begin{enumerate}
  \item Veľké pamäťové nároky
  \item Nutnosť navštíviť a správne spočítať Q pre všetky $s(n)$, $a(n)$
\end{enumerate}
Prvý problém nepredstavuje pre súčasné počítače až tak veľký nedostatok tabuľkového
riešenia. Horšia je situácia v prípade vypĺňania korektných hodnôt v tabuľke. Práve rekurentnou
povahou algoritmov Q-learning a SARSA je časovo veľmi náročné vyplniť tieto hodnoty -
mnohonásobne treba navštíviť všetky stavy a vykonať v nich všetky akcie. Práve to je
primárny dôvod aproximovať funkciu $Q(s(n), a(n))$.
