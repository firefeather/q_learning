%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}                                \label{literatura}
%\addcontentsline{toc}{section}{Literatúra}
\addcontentsline{toc}{chapter}{Literatúra}

\bibitem{bib:adaptive_01} Nhan Nguyen, NASA Ames Research Center, Moffett Field, CA 94035 :
Predictor-Model-Based Least-Squares Model-Reference Adaptive Control with Chebyshev Orthogonal Polynomial Approximation

\bibitem{bib:adaptive_02} Girish Chowdhary and Eric Johnson, Least  Squares  Based  Modification  for  Adaptive  Control
\url{http://web.mit.edu/girishc/www/publications/files/Chow_Joh_CDC_10_ls.pdf}

\bibitem{bib:adaptive_03}
Sun Pei, Noise Resistant Least Squares Based Adaptive Control, March 27, 2012, Stockholm, Sweden
\url{http://www.diva-portal.org/smash/get/diva2:514116/FULLTEXT01.pdf}


\bibitem{bib:gradient_01} Prof. Nathan L. Gibson Department of Mathematics,
Gradient-based Methods for Optimization. Part I., 2011
\url{http://math.oregonstate.edu/~gibsonn/optpart1.pdf}

\bibitem{bib:gradient_02} Antony Jameson,
Department of Aeronautics and Astronautics
Stanford University, Stanford, CA 94305-4035
Gradient Based Optimization Methods,
\url{http://aero-comlab.stanford.edu/Papers/jameson.gbom.pdf}

\bibitem{bib:gradient_03} L. Hasdorff, Gradient optimization and nonlinear control,
ISBN	0471358703, \url{https://books.google.cz/books?id=o\_ZQAAAAMAAJ}

\bibitem{bib:ilc_01} Kevin L. Moore, Iterative Learning Control,
\url{http://inside.mines.edu/~kmoore/survey.pdf}

\bibitem{bib:ilc_02}  Kevin L. Moore, An Introduction to Iterative Learning Control Theory,
\url{http://inside.mines.edu/~kmoore/504_ILC_Seminar-Save.pdf}

\bibitem{bib:gradient_04} Jeff Heaton, Introduction to Neural Networks with Java,
Heaton Research, Inc., 2008, ISBN	1604390085


\bibitem{bib:q_learning_watkins}
CHRISTOPHER  J.C.H. WATKINS, PETER DAYAN : Technical Note Q-Learning,
Machine Learning,  8,279-292 (1992)
\url{http://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf}

\bibitem{bib:q_tutorial_01} Q-learning 1
\url{https://www-s.acm.illinois.edu/sigart/docs/QLearning.pdf}

\bibitem{bib:q_tutorial_02} Q-learning 2
\url{http://mnemstudio.org/path-finding-q-learning-tutorial.htm}


\bibitem{bib:q_proof_01} Francisco S. Melo
Institute for Systems and Robotics,
Instituto Superior Técnico,
Lisboa, PORTUGAL : Convergence of Q-learning:  a simple proof
\url{http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf}

\bibitem{bib:q_proof_02} Eyal Even-Dar, Yishay Mansour :
Convergence of optimistic and incremental Q-learning,
\url{http://web.cs.iastate.edu/~honavar/rl-optimistic.pdf}


\bibitem{bib:q_proof_03}
Carden, Stephen, "Convergence of a Reinforcement Learning Algorithm in Continuous Domains" (2014).
All Dissertations. Paper 1325.
\url{http://tigerprints.clemson.edu/cgi/viewcontent.cgi?article=2326&context=all_dissertations}

\bibitem{bib:q_proof_04} Francisco S. Melo and M. Isabel Ribeiro,
Convergence of Q-learning with linear function approximation,
Proceedings of the European Control Conference 2007 Kos, Greece, July 2-5, 2007,
\url{http://gaips.inesc-id.pt/~fmelo/pub/melo07ecc.pdf}

\bibitem{bib:sarsa} Karan M. Gupta Department of Computer Science Texas TechUniversity Lubbock, TX 79409-3104 :
Performance Comparison of Sarsa($\lambda$) and Watkin’s Q($\lambda$) Algorithms,
\url{http://www.karanmg.net/Computers/reinforcementLearning/finalProject/KaranComparisonOfSarsaWatkins.pdf}


\bibitem{bib:kohonen_01} R. Rojas: Neural Networks, Springer-Verlag, Berlin, 1996, Kohonen Networks
\url{https://page.mi.fu-berlin.de/rojas/neural/chapter/K15.pdf}

\bibitem{bib:kohonen_02} Steven K. Rogers, Matthew Kabrisky
SPIE Press, 1991, ISBN	0819405345 : An Introduction to Biological and Artificial Neural Networks for Pattern Recognition
\url{https://books.google.cz/books?id=uo4Smk6QnTgC}

\bibitem{bib:kohonen_03} Teuvo Kohonen and Timo Honkela (2007), Scholarpedia, 2(1):1568 : Kohonen network
\url{http://www.scholarpedia.org/article/Kohonen_network}

\bibitem{bib:markov_01} Markovove rozhodovacie procesy, stručne :
Pieter Abbeel  UC Berkeley EECS : Markov Decision Processes and Exact Solution Methods
\url{http://www.cs.berkeley.edu/~pabbeel/cs287-fa12/slides/mdps-exact-methods.pdf}

\bibitem{bib:markov_02} Martin L. Puterman : Markov Decision Processes: Discrete Stochastic Dynamic Programming
, isbn 9781118625873, rok 2014, \url{https://books.google.sk/books?id=VvBjBAAAQBAJ}

\bibitem{bib:nano_q_link}
NanoQ learning zdrojové súbory\url{https://github.com/michalnand/q_learning/tree/master/src/nano_q_learning}

\bibitem{bib:kolomongorov_01}
Fundamentals of Artificial Neural Networks Mohamad H. Hassoun, MIT Press, 1995

\bibitem{bib:kolomongorov_02}
B. Irie Auditory \& Visual Perception Res. Lab., ATR, Osaka, Japan, S. Miyake :
Neural Networks, 1988., IEEE International Conference on, INSPEC 3350063

\bibitem{bib:kolomongorov_03}
Kolomongorov teorém, stručne
\url{https://en.wikipedia.org/wiki/Universal_approximation_theorem}

\bibitem{bib:backpropagation_00}
R. Rojas: Neural Networks, Springer-Verlag, Berlin, 1996, chap 7

\bibitem{bib:backpropagation_01}
Martin Riedmiller,  Computer Standards \& Interfaces Volume 16, Issue 3, July 1994, Pages 265-278 :
Advanced supervised learning in multi-layer perceptrons — From backpropagation to adaptive learning algorithms

\bibitem{bib:backpropagation_02}
J. Leonard, M.A. Kramer, Computers \& Chemical Engineering Volume 14, Issue 3, March 1990, Pages 337–341
Improvement of the backpropagation algorithm for training neural networks

\bibitem{bib:annealing_01} Jonathan Engel,
Norman Bridge Laboratoryof Plly sics 161-33, California Institute of Technology,
Pasadena, CA 91125, USA : Teaching Feed-Forward Neural Networks by Simulated Annealing



\bibitem{bib:aproximation_01} Francisco S. Melo, Sean P. Meyn, M. Isabel Ribeiro
An Analysis of Reinforcement Learning with Function Approximation,
Appearing in Proceedings of the 25th International Conference on Machine Learning, Helsinki, Finland, 2008
\url{http://www.machinelearning.org/archive/icml2008/papers/652.pdf}

\bibitem{bib:aproximation_02} David Silver : Lecture 6: Value Function Approximation
\url{http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf}

\bibitem{bib:aproximation_03} Francisco S. Melo M. Isabel Ribeiro : Q-learning with linear function approximation
\url{http://gaips.inesc-id.pt/~fmelo/pub/melo07tr-b.pdf}

\bibitem{bib:aproximation_04} Marina Irodova and Robert H. Sloan : Reinforcement Learning and Function Approximation,
2005,  American  Association  for  Artificial  Intelli-gence (www.aaai.org)
\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.7833&rep=rep1&type=pdf}



\bibitem{bib:strategy_01} Punit Pandey, Dr. Shishir Kumar, DeepshikhaPandey,
Reinforcement Learning by Comparing Immediate Reward, (IJCSIS)
International Journal of Computer Science and Information Security, Vol. 8
, No. 5, August 2010
\url{http://arxiv.org/pdf/1009.2566.pdf}

\bibitem{bib:strategy_02} Melanie Coggan, CRA-W DMP Project at McGill University (2004) :
Exploration and Exploitation inReinforcement Learning
\url{http://ftp.bstu.by/ai/To-dom/My_research/Papers-2.1-done/RL/0/FinalReport.pdf}

\bibitem{bib:strategy_03} Mark Humphrys Trinity Hall, University of Cambridge August 1996,
\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.8309&rep=rep1&type=pdf}

\bibitem{bib:q_app_01}
Jinyi Yao Dept. of Comput. Sci. \& Technol., Tsinghua Univ., Beijing, China Jiang Chen ; Zengqi Sun
An application in RoboCup combining Q-learning with adversarial planning, 2002
\url{http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1022159&abstractAccess=no&userType=inst}


\bibitem{bib:q_app_02} Asma Al-Tamimi, Frank L. Lewis , Murad Abu-Khalaf
Model-free Q-learning designs for linear discrete-time zero-sum games with application to H-infinity control, 2006
\url{http://www.sciencedirect.com/science/article/pii/S0005109806004249}

\bibitem{bib:q_app_03} Asma Al-Tamimi, Frank L. Lewis , Murad Abu-Khalaf
Model-free Q-learning designs for linear discrete-time zero-sum games with application to H-infinity control, 2006
\url{http://www.sciencedirect.com/science/article/pii/S0005109806004249}

\bibitem{bib:q_conv_proof}
Christopher J. C. H. Watkins, Peter Dayan
Q-learning, 1992
\url{http://link.springer.com/article/10.1007/BF00992698}

\bibitem{bib:q_fnn_problem}
Mae L. Seto Springer Science \& Business Media, 9. 12. 2012,
Marine Robot Autonomy, ISBN	1461456592, chap 7.3.3.2

\bibitem{bib:reinforcement_leraning_01}
Peter Dayan, Christopher J.C.H. Watkins,
Reinforcement Learning
\url{http://www.gatsby.ucl.ac.uk/~dayan/papers/dw01.pdf}


\bibitem{bib:reinforcement_leraning_02}
Daniel Dewey, Oxford Martin Programme on the Impacts of Future Technology,
Future of Humanity Institute :
Reinforcement Learning and the Reward Engineering Principle
\url{http://www.danieldewey.net/reward-engineering-principle.pdf}

\bibitem{bib:mototko_video} video robota Motoko Aftermath
Michal Chovanec, youtube
\url{https://www.youtube.com/watch?v=8sskJN_zuko}

\end{thebibliography}
