%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{thebibliography}{99}                                \label{literatura}
%\addcontentsline{toc}{section}{Literatúra}
\addcontentsline{toc}{chapter}{Literatúra}

\bibitem{bib:nano_q_link}
NanoQ learning zdrojové súbory\url{https://github.com/michalnand/q_learning/tree/master/src/nano_q_learning}

\bibitem{bib:kolomongorov_01}
Fundamentals of Artificial Neural Networks Mohamad H. Hassoun, MIT Press, 1995

\bibitem{bib:kolomongorov_02}
B. Irie Auditory \& Visual Perception Res. Lab., ATR, Osaka, Japan, S. Miyake :
Neural Networks, 1988., IEEE International Conference on, INSPEC 3350063

\bibitem{bib:kolomongorov_03}
Kolomongorov teorém, stručne
\url{https://en.wikipedia.org/wiki/Universal_approximation_theorem}

\bibitem{bib:backpropagation_00}
R. Rojas: Neural Networks, Springer-Verlag, Berlin, 1996, chap 7

\bibitem{bib:backpropagation_01}
Martin Riedmiller,  Computer Standards \& Interfaces Volume 16, Issue 3, July 1994, Pages 265-278 :
Advanced supervised learning in multi-layer perceptrons — From backpropagation to adaptive learning algorithms

\bibitem{bib:backpropagation_02}
J. Leonard, M.A. Kramer, Computers \& Chemical Engineering Volume 14, Issue 3, March 1990, Pages 337–341
Improvement of the backpropagation algorithm for training neural networks

\bibitem{bib:annealing_01} Jonathan Engel,
Norman Bridge Laboratoryof Plly sics 161-33, California Institute of Technology,
Pasadena, CA 91125, USA : Teaching Feed-Forward Neural Networks by Simulated Annealing

\end{thebibliography}
