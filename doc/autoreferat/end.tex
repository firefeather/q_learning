\chapter{Záver}

Práca rieši problematiku aproximovania funkcie ohodnotení v algoritmoch Q-learning.
S pomedzi najčastejšie používaných prístupov bola zvolená aproximácia neurónovou
sieťou pomocou bázických funkcií. Oproti bežne používanému prístupu lineárnej kombinácií
príznakov (features) sa líši tým, že samotné tvary príznaky si algoritmus stanovuje sám,
počas učenia. Zmenšuje sa teda potrebná znalosť programátora.

{\bf Vedecký prínos} je možné nájsť v
\begin{itemize}
  \item Ukážka nevhodnosti použitia doprednej siete v predloženom probléme učenou
  gradientovými metódami. Riešenie nekonvergovalo ani po miliónoch iteráciach v triviálnom
  experimente s dvoma akciami. Príčinou je nelokálnosť učenia siete - zmena hodnoty v jednom bode,
  zmení hodnoty v každom bode, a nie nutne k lepšiemu. Od siete sa súčastne požaduje generovanie
  správnej hodnoty aj učenie v nejakom inom bode.
  \item Uvedenie algoritmu nanoQ, ktorý vyšetruje systém s jedným stavom. Nepodarilo sa nájsť
  publikáciu ktorá by tento princíp využívala. Algoritmus môže nájsť uplatnenie v riešení pohybu
  jednoduchého robota.
  \item Uvedenie novej bázickej funkcie, ktorá z testovaných najlepšie aproximuje funkciu ohodnotení.
  Táto funkcia môže byť učená lokálne, a vďaka časti $P(s(n), a(n))$ umožňuje zabezpečiť potrebnú
  strmosť, bez nutnosti širokého rozsahu parametrov $\beta$ v časti $H(s(n), a(n))$ - ten
  môže zostať malý, a riešiť tak šírenie kladnej odmeny na ďalšie stavy v súlade s parametrom
  $\gamma$.
  \item Testovanie Q-learing algoritmu na reálnom robotovi, kde predstavuje druhú úroveň
  riadenia. Na spodnej vrstve sa pravuje s PID regulátormi, na druhej sa pomocou Q-learning
  algoritmu stanovujú žiadané hodnoty.
\end{itemize}

Napriek uvedeným skutočnostiam a súčastnému stavu komerčnej sféry, autor práce nepredpokladá
využitie Q-learning algoritmov v priemyselnej praxi. Medzi hlavné dôvody možno zaradiť
konzervatívny prístup riadenia v priemysle, kde väčšinu úloh plnohodnotne vyrieši PID
regulátor a nad ním postavená logika vetvenia (napr. rôzne stavové automaty).
Práca tak predstavuje nepatrný prínos v teoretickej oblasti reinforcement learning algoritmov.
Jediné možné využitie v blízkej dobe je možné nájsť v počítačových hrách.
Všetky zdrojové súbory a podrobné výsledky experimentov (vrátane dát na ďalšie smerovanie)
sú k dispizícií pod GNU GPL licenciou. Práca tak spadá do kategórie otvorenej vedy.
Zdrojové súbory pre Q-learning experiment sú k dispozícií na autorovom gite \cite{bib:q_learning_git}.
Spolu je to cca 55648 súborov, z toho cca. 17000 pripadá na výsledky experimentov a cca 36000 na
 zdrojové súbory. Zdrojové súbory (vrátane podkladov na výrobu) pre robota Motoko sú k dispozícií na
 \cite{bib:motoko_git}.
